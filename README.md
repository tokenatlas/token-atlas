# token-atlas

This document aims to provide a comprehensive list of large open-source text datasets which can be used for pretraining LLMs. Large in this context means >100 GB of text. This can (ideally) be a useful resource for researchers who want to quickly get an overview of open-source efforts. This is especially relevant as there are multiple public projects that sometimes overlap. The document is currently not very comprehensive, so any help in listing datasets is appreciated.


# datasets

| name        | size |   paper | link                                                    |
|-------------|------|--------------------|-------------------------------------|
| the pile    |   825 GB  | https://arxiv.org/abs/2101.00027 | https://pile.eleuther.ai/                               |
| c4 (en)     |  807 GB   | https://arxiv.org/abs/1910.10683 | https://www.tensorflow.org/datasets/catalog/c4          |
| pile of law | 256 GB    | https://arxiv.org/abs/2207.00220 |https://huggingface.co/datasets/pile-of-law/pile-of-law |
| Pushshift Reddit | 256 GB  | https://arxiv.org/abs/2001.08435 | https://files.pushshift.io/reddit/comments/ |
| ROOTS corpus | 1.6 TB | https://openreview.net/forum?id=UoEw6KigkUn | https://huggingface.co/bigscience-data |